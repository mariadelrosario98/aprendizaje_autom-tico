{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1EFk7rNkK9fcNrql1p6B1cSz6eO0bzxJc","authorship_tag":"ABX9TyPFZHI8wuYujNaX1g2lJ1ZW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jgFKRpxY9xjc","executionInfo":{"status":"ok","timestamp":1740019445316,"user_tz":360,"elapsed":3735,"user":{"displayName":"Maria Del Rosario Castro","userId":"02696862558090213229"}},"outputId":"e751df0f-8f85-433c-a972-33bf4b3f2a2a"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-02-20 02:44:01--  https://docs.google.com/uc?export=download&id=1-03DcqzYIYtIAIt188ujy5Wr4GSdlgv5\n","Resolving docs.google.com (docs.google.com)... 64.233.189.138, 64.233.189.113, 64.233.189.139, ...\n","Connecting to docs.google.com (docs.google.com)|64.233.189.138|:443... connected.\n","HTTP request sent, awaiting response... 303 See Other\n","Location: https://drive.usercontent.google.com/download?id=1-03DcqzYIYtIAIt188ujy5Wr4GSdlgv5&export=download [following]\n","--2025-02-20 02:44:01--  https://drive.usercontent.google.com/download?id=1-03DcqzYIYtIAIt188ujy5Wr4GSdlgv5&export=download\n","Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 173.194.174.132, 2404:6800:4008:c1b::84\n","Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|173.194.174.132|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 8358 (8.2K) [application/octet-stream]\n","Saving to: ‚Äòloanpred_test.csv‚Äô\n","\n","loanpred_test.csv   100%[===================>]   8.16K  --.-KB/s    in 0s      \n","\n","2025-02-20 02:44:03 (48.1 MB/s) - ‚Äòloanpred_test.csv‚Äô saved [8358/8358]\n","\n","\n","üöÄ Training Random Forest...\n","Random Forest Validation Accuracy: 0.8228\n","Random Forest Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.83      0.45      0.59        22\n","           1       0.82      0.96      0.89        57\n","\n","    accuracy                           0.82        79\n","   macro avg       0.83      0.71      0.74        79\n","weighted avg       0.82      0.82      0.80        79\n","\n","\n","üöÄ Training Bagging...\n","Bagging Validation Accuracy: 0.8354\n","Bagging Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.41      0.58        22\n","           1       0.81      1.00      0.90        57\n","\n","    accuracy                           0.84        79\n","   macro avg       0.91      0.70      0.74        79\n","weighted avg       0.87      0.84      0.81        79\n","\n","\n","üöÄ Training Boosting...\n","Boosting Validation Accuracy: 0.8228\n","Boosting Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.72      0.59      0.65        22\n","           1       0.85      0.91      0.88        57\n","\n","    accuracy                           0.82        79\n","   macro avg       0.79      0.75      0.77        79\n","weighted avg       0.82      0.82      0.82        79\n","\n","‚úÖ Predictions saved to loan_predictions.csv\n"]}],"source":["# Import necessary libraries\n","import pandas as pd\n","import numpy as np\n","from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","import joblib\n","\n","# Step 1: Load Data\n","# Download loanpred_test.csv\n","!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1-03DcqzYIYtIAIt188ujy5Wr4GSdlgv5' -O loanpred_test.csv\n","\n","# Load training and test data\n","df_train = pd.read_csv(\"/content/drive/MyDrive/EAFIT/SEGUNDO SEMESTRE/APRENDIZAJE AUTOM√ÅTICO/COMPETITION 1/train_file.csv\")\n","df_test = pd.read_csv(\"/content/loanpred_test.csv\")\n","\n","# Step 2: Data Preprocessing\n","# Define features and target\n","X = df_train.drop(columns=[\"Loan_Status\", \"Loan_ID\"])\n","y = df_train[\"Loan_Status\"]\n","\n","# Identify numerical and categorical columns\n","num_cols = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History']\n","cat_cols = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area']\n","\n","# Preprocessing pipelines for numerical and categorical data\n","num_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='median')),\n","    ('scaler', StandardScaler())\n","])\n","\n","cat_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='most_frequent')),\n","    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n","])\n","\n","# Combine transformers into a preprocessor\n","preprocessor = ColumnTransformer(transformers=[\n","    ('num', num_transformer, num_cols),\n","    ('cat', cat_transformer, cat_cols)\n","])\n","\n","# Step 3: Define Models\n","\n","# Random Forest\n","rf_clf = RandomForestClassifier(n_estimators=7, max_depth=7, min_samples_split=7, random_state=7)\n","\n","# Bagging Classifier (using Decision Trees)\n","bagging_clf = BaggingClassifier(\n","    estimator=rf_clf,  # Updated from base_estimator to estimator\n","    n_estimators=50,\n","    random_state=42\n",")\n","\n","# Gradient Boosting Classifier\n","gb_clf = GradientBoostingClassifier(\n","    n_estimators=100,\n","    learning_rate=0.1,\n","    max_depth=5,\n","    random_state=42\n",")\n","\n","# Step 4: Build Pipelines for Each Model\n","\n","# Random Forest Pipeline\n","rf_pipeline = Pipeline(steps=[\n","    (\"preprocessor\", preprocessor),\n","    (\"classifier\", rf_clf)\n","])\n","\n","# Bagging Pipeline\n","bagging_pipeline = Pipeline(steps=[\n","    (\"preprocessor\", preprocessor),\n","    (\"classifier\", bagging_clf)\n","])\n","\n","# Boosting Pipeline\n","boosting_pipeline = Pipeline(steps=[\n","    (\"preprocessor\", preprocessor),\n","    (\"classifier\", gb_clf)\n","])\n","\n","# Step 5: Train and Evaluate Models\n","models = {\n","    \"Random Forest\": rf_pipeline,\n","    \"Bagging\": bagging_pipeline,\n","    \"Boosting\": boosting_pipeline\n","}\n","\n","# Train-test split for validation\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","for name, pipeline in models.items():\n","    print(f\"\\nüöÄ Training {name}...\")\n","    pipeline.fit(X_train, y_train)\n","    y_pred_val = pipeline.predict(X_val)\n","    print(f\"{name} Validation Accuracy: {accuracy_score(y_val, y_pred_val):.4f}\")\n","    print(f\"{name} Classification Report:\\n{classification_report(y_val, y_pred_val)}\")\n","\n","# Step 6: Generate Predictions on Test Data using the Best Model (e.g., Boosting)\n","best_pipeline = boosting_pipeline  # You can switch to bagging_pipeline or rf_pipeline\n","\n","X_test = df_test.drop(columns=[\"Loan_ID\"], errors=\"ignore\")\n","predictions = best_pipeline.predict(X_test)\n","\n","# Step 7: Prepare Submission File\n","submission = pd.DataFrame({\n","    \"Loan_ID\": df_test[\"Loan_ID\"],\n","    \"Loan_Status\": predictions\n","})\n","\n","# Map numerical predictions to labels if necessary\n","submission[\"Loan_Status\"] = submission[\"Loan_Status\"].map({1: \"Y\", 0: \"N\"})\n","\n","# Save to CSV\n","submission.to_csv(\"loan_predictions.csv\", index=False)\n","print(\"‚úÖ Predictions saved to loan_predictions.csv\")\n","\n"]}]}